import streamlit as st
import google.generativeai as genai
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound, VideoUnavailable
import json
import os
from datetime import datetime, timedelta
import requests
import urllib.parse
import pandas as pd
import plotly.graph_objects as go
import yfinance as yf
import random
from apify_client import ApifyClient

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="Searching Architecture", page_icon="ğŸ—ï¸", layout="wide")

# API í‚¤ ì„¤ì •
genai.configure(api_key=st.secrets["GOOGLE_AI_STUDIO_API_KEY"])
YOUTUBE_API_KEYS = [
    st.secrets["YOUTUBE_API_KEY1"],
    st.secrets["YOUTUBE_API_KEY2"],
    st.secrets["YOUTUBE_API_KEY3"],
    st.secrets["YOUTUBE_API_KEY4"]
]
APIFY_API_KEY = st.secrets["APIFY_API_KEY"]
apify_client = ApifyClient(APIFY_API_KEY)

# ê±´ì¶•/ê±´ì„¤ ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ì •ì˜
ARCH_DOMAINS = {
    "ê±´ì¶•ê³„íš": ["ê±´ì¶•", "ì„¤ê³„", "ê±´ì¶•ë””ìì¸", "ê³µê³µê±´ì¶•", "íŒ¨ì‹œë¸Œí•˜ìš°ìŠ¤", "ì œë¡œì—ë„ˆì§€"],
    "ë„ì‹œì¬ìƒ": ["ë„ì‹œì¬ìƒ", "ë¦¬ëª¨ë¸ë§", "ì¬ê±´ì¶•", "ì¬ê°œë°œ", "ë…¸í›„ ê±´ì¶•ë¬¼", "ì—­ì„¸ê¶Œ ê°œë°œ"],
    "ê±´ì„¤ê¸°ìˆ ": ["ìŠ¤ë§ˆíŠ¸ê±´ì„¤", "BIM", "ë“œë¡ ê±´ì„¤", "ëª¨ë“ˆëŸ¬", "ê±´ì„¤ë¡œë´‡", "3Dí”„ë¦°íŒ… ê±´ì¶•"],
    "ê±´ì¶•ì •ì±…": ["ê±´ì¶•ë²•", "ê±´ì¶• ê·œì œ", "ê±´ì„¤ì•ˆì „", "ì—ë„ˆì§€ ì¸ì¦ì œ", "ë…¹ìƒ‰ê±´ì¶•"],
    "ì¹œí™”ê²½ê±´ì¶•": ["ì¹œí™˜ê²½ê±´ì¶•", "ê·¸ë¦°ë¹Œë”©", "ESG ê±´ì¶•", "LEED", "ì œë¡œì—ë„ˆì§€ê±´ì¶•"]
}

MAJOR_PROJECTS = [
    "ì„¸ìš´ì¬ì •ë¹„ì´‰ì§„ì§€êµ¬", "ê´‘ìš´ëŒ€ ì—­ì„¸ê¶Œ ê°œë°œ", "ì†¡ë„ êµ­ì œë„ì‹œ", "ìš©ì‚°êµ­ì œì—…ë¬´ì§€êµ¬", "ìœ„ë¡€ ì‹ ë„ì‹œ"
]

# ì „ì—­ ë³€ìˆ˜ë¡œ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ API í‚¤ ì¸ë±ìŠ¤ ì¶”ì 
current_api_key_index = 0

# ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜ (Serp API ì‚¬ìš©)
def search_news(domain, additional_query, published_after, max_results=10):
    global current_api_key_index
    
    # API í‚¤ ë²ˆê°ˆì•„ ì‚¬ìš©
    api_keys = [st.secrets["SERP_API_KEY1"], st.secrets["SERP_API_KEY2"]]
    api_key = api_keys[current_api_key_index]
    current_api_key_index = (current_api_key_index + 1) % len(api_keys)
    
    keywords = " OR ".join(ARCH_DOMAINS[domain])
    
    if additional_query:
        query = f"({keywords}) AND ({additional_query})"
    else:
        query = keywords
    
    encoded_query = urllib.parse.quote(query)
    
    url = f"https://serpapi.com/search.json?q={encoded_query}&tbm=nws&api_key={api_key}&num={max_results}&sort=date"
    
    if published_after:
        url += f"&tbs=qdr:{published_after}"
    
    response = requests.get(url)
    news_data = response.json()
    articles = news_data.get('news_results', [])
    
    unique_articles = []
    seen_urls = set()
    for article in articles:
        if article['link'] not in seen_urls:
            unique_articles.append({
                'title': article.get('title', ''),
                'source': {'name': article.get('source', '')},
                'description': article.get('snippet', ''),
                'url': article.get('link', ''),
                'content': article.get('snippet', '')
            })
            seen_urls.add(article['link'])
        if len(unique_articles) == max_results:
            break
    
    return unique_articles

# YouTube ê²€ìƒ‰ í•¨ìˆ˜
def search_videos_with_transcript(domain, additional_query, published_after, max_results=10):
    try:
        YOUTUBE_API_KEY = random.choice(YOUTUBE_API_KEYS)
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        keywords = " OR ".join(ARCH_DOMAINS[domain])
        query = f"({keywords}) {additional_query}".strip()
        
        # st.write(f"ê²€ìƒ‰ ì¿¼ë¦¬: {query}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        request = youtube.search().list(
            q=query,
            type='video',
            part='id,snippet',
            order='relevance',
            publishedAfter=published_after,
            maxResults=max_results
        )
        response = request.execute()

        videos_with_transcript = []
        for item in response['items']:
            video_id = item['id']['videoId']
            # if get_video_transcript(video_id):  # ìë§‰ ìˆëŠ” ì˜ìƒë§Œ í•„í„°ë§
            videos_with_transcript.append(item)
        
        # st.write(f"ìë§‰ì´ ìˆëŠ” ë¹„ë””ì˜¤ ìˆ˜: {len(videos_with_transcript)}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        return videos_with_transcript[:max_results], len(response['items'])
    except Exception as e:
        st.error(f"YouTube ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return [], 0


# ì¡°íšŒ ê¸°ê°„ ì„ íƒ í•¨ìˆ˜
def get_published_after(option):
    today = datetime.utcnow() + timedelta(hours=9)  # UTC ì‹œê°„ì„ KSTë¡œ ë³€í™˜ (+9ì‹œê°„)
    if option == "ìµœê·¼ 1ì¼":
        return (today - timedelta(days=1)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 1ì£¼ì¼":
        return (today - timedelta(weeks=1)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 1ê°œì›”":
        return (today - timedelta(weeks=4)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 3ê°œì›”":
        return (today - timedelta(weeks=12)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 6ê°œì›”":
        return (today - timedelta(weeks=24)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 1ë…„":
        return (today - timedelta(weeks=52)).isoformat("T") + "Z"
    else:
        return None  # ì´ ê²½ìš° ì¡°íšŒ ê¸°ê°„ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

# ìë§‰ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_video_transcript(video_id):
    try:
        # 1. youtube-transcript-apië¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
        transcript_text = ' '.join([entry['text'] for entry in transcript_list])
        return transcript_text
    except (TranscriptsDisabled, NoTranscriptFound):
        # ìë§‰ì´ ì—†ê±°ë‚˜ ë¹„í™œì„±í™”ëœ ê²½ìš° Apify ì‚¬ìš©
        pass

    # 2. Apifyë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ ê°€ì ¸ì˜¤ê¸° ì‹œë„
    video_url = f"https://www.youtube.com/watch?v={video_id}"
    try:
        run_input = {
            "startUrls": [video_url]
        }
        run = apify_client.actor("topaz_sharingan/Youtube-Transcript-Scraper-1").call(run_input=run_input)
        for item in apify_client.dataset(run["defaultDatasetId"]).iterate_items():
            if item.get("transcript"):
                return item["transcript"]
    except Exception as e:
        pass

    return None


# ë¹„ë””ì˜¤ ì„¤ëª…ê³¼ ëŒ“ê¸€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_video_info(video_id):
    try:
        YOUTUBE_API_KEY = random.choice(YOUTUBE_API_KEYS)
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        
        # ë¹„ë””ì˜¤ ì„¤ëª… ê°€ì ¸ì˜¤ê¸°
        video_request = youtube.videos().list(
            part="snippet",
            id=video_id
        )
        video_response = video_request.execute()
        description = video_response['items'][0]['snippet']['description'] if video_response['items'] else None
        
        # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
        comments_request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            textFormat="plainText",
            maxResults=30  # ìƒìœ„ 30ê°œ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
        )
        comments_response = comments_request.execute()
        comments = [item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in comments_response['items']]
        
        return {
            'description': description,
            'comments': comments
        }
    except Exception as e:
        pass
        return None

# YouTube ì˜ìƒ ìš”ì•½ í•¨ìˆ˜
def summarize_video(video_id, video_title):
    try:
        transcript = get_video_transcript(video_id)
        video_info = get_video_info(video_id)
        
        if not transcript and not video_info:
            return "ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        content = f"ì œëª©: {video_title}\n\n"
        
        if transcript:
            content += f"ìë§‰ ë‚´ìš©:\n{transcript}\n\n"
        
        if video_info:
            if video_info.get('description'):
                content += f"ë¹„ë””ì˜¤ ì„¤ëª…:\n{video_info['description']}\n\n"
            
            if video_info.get('comments'):
                content += "ì£¼ìš” ëŒ“ê¸€:\n"
                for comment in video_info['comments']:
                    content += f"- {comment}\n"
                content += "\n"
        
        prompt = f"""ë‹¤ìŒ YouTube ì˜ìƒì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ë…ì„± ìˆëŠ” í•œ í˜ì´ì§€ì˜ ë³´ê³ ì„œ í˜•íƒœë¡œ ìš”ì•½í•˜ì„¸ìš”. ìµœì¢… ê²°ê³¼ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ìë§‰ ë‚´ìš©ì´ ë©”ì¸ ì •ë³´ì´ê³ , ë¹„ë””ì˜¤ ì„¤ëª…ê³¼ ì£¼ìš” ëŒ“ê¸€ì€ ì°¸ê³  ì •ë³´ì…ë‹ˆë‹¤.

ë³´ê³ ì„œ êµ¬ì¡°:
1. ì˜ìƒ ê°œìš”
2. ì£¼ìš” ë‚´ìš©
3. ì‹œì²­ì ë°˜ì‘ (ëŒ“ê¸€ ê¸°ë°˜)
4. ê²°ë¡  ë° ì‹œì‚¬ì 
ì˜ìƒ ì •ë³´:
{content}"""
        response = model.generate_content(prompt)
        if not response or not response.parts:
            feedback = response.prompt_feedback if response else "No response received."
            return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {feedback}"
        summary = response.text
        return summary
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# ë‰´ìŠ¤ ê¸°ì‚¬ ì¢…í•© ë¶„ì„ í•¨ìˆ˜
def analyze_news_articles(articles):
    try:
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # ëª¨ë“  ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        all_articles = "\n\n".join([f"ì œëª©: {article['title']}\në‚´ìš©: {article['content']}" for article in articles])
        
        prompt = f"""
ë‹¤ìŒì€ íŠ¹ì • ì£¼ì œì— ê´€í•œ ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ê°€ë…ì„± ìˆëŠ” í•œ í˜ì´ì§€ì˜ ë³´ê³ ì„œë¥¼ ë‹¤ìŒ í˜•ì‹ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. ì£¼ìš” ì´ìŠˆ ìš”ì•½ (3-5ê°œì˜ í•µì‹¬ í¬ì¸íŠ¸)
2. ìƒì„¸ ë¶„ì„ (ê° ì£¼ìš” ì´ìŠˆì— ëŒ€í•œ ì‹¬ì¸µ ì„¤ëª…)
3. ë‹¤ì–‘í•œ ê´€ì  (ê¸°ì‚¬ë“¤ì—ì„œ ë‚˜íƒ€ë‚œ ì„œë¡œ ë‹¤ë¥¸ ì˜ê²¬ì´ë‚˜ í•´ì„)
4. ì‹œì‚¬ì  ë° í–¥í›„ ì „ë§

ë³´ê³ ì„œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë¶„ì„ ì‹œ ê°ê´€ì„±ì„ ìœ ì§€í•˜ê³ , í¸í–¥ëœ ì˜ê²¬ì„ ì œì‹œí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ë‚´ìš©:
{all_articles}
"""
        response = model.generate_content(prompt)

        if not response or not response.parts:
            feedback = response.prompt_feedback if response else "No response received."
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {feedback}"

        analysis = response.text
        return analysis
    except Exception as e:
        return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



# íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
def download_summary_file(summary_text, file_name="summary.txt"):
    st.download_button(
        label="ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
        data=summary_text,
        file_name=file_name,
        mime="text/plain"
    )

# ê²€ìƒ‰ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def execute_search():
    st.session_state['search_executed'] = True
    source = st.session_state.get('source')
    if source in ["YouTube", "ë‰´ìŠ¤"]:
        published_after = get_published_after(st.session_state['period'])
        
        if source == "YouTube":
            # YouTube ì˜ìƒ ê²€ìƒ‰
            with st.spinner(f"{source}ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                videos, total_video_results = search_videos_with_transcript(
                    st.session_state['domain'], 
                    st.session_state['additional_query'], 
                    published_after)
                st.session_state.search_results = {'videos': videos, 'news': [], 'financial_info': {}}
                st.session_state.total_results = total_video_results
                st.session_state.summary = ""  # YouTube ê²€ìƒ‰ ì‹œ ìš”ì•½ ì´ˆê¸°í™”
        
        elif source == "ë‰´ìŠ¤":
            # ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìë™ ë¶„ì„
            with st.spinner(f"{source}ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                news_articles = search_news(
                    st.session_state['domain'], 
                    st.session_state['additional_query'], 
                    published_after, 
                    max_results=10)
                total_news_results = len(news_articles)
                st.session_state.search_results = {'videos': [], 'news': news_articles, 'financial_info': {}}
                st.session_state.total_results = total_news_results
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ ìë™ ë¶„ì„
                with st.spinner("ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì¢…í•© ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                    st.session_state.summary = analyze_news_articles(news_articles)
        
        if not st.session_state.total_results:
            st.warning(f"{source}ì—ì„œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë„ë©”ì¸ì´ë‚˜ ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")


# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'search_executed' not in st.session_state:
    st.session_state['search_executed'] = False
if 'search_results' not in st.session_state:
    st.session_state['search_results'] = {'videos': [], 'news': []}
    st.session_state['total_results'] = 0
if 'summary' not in st.session_state:
    st.session_state['summary'] = ""

# Streamlit ì•±
st.markdown('<h1>ğŸ¤– ê±´ì¶•/ê±´ì„¤ AI ì„œë¹„ìŠ¤ í”Œë«í¼ <span style="color:red">AI</span>senet</h1>', unsafe_allow_html=True)
st.markdown("ì´ ì„œë¹„ìŠ¤ëŠ” ì„ íƒí•œ ê±´ì¶•/ê±´ì„¤ì„¤ ë„ë©”ì¸ì— ëŒ€í•œ YouTube ì˜ìƒ, ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  AIë¥¼ ì´ìš©í•´ ë¶„ì„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ê²€ìƒ‰ ì¡°ê±´ì„ ì„ íƒí•˜ê³  ê²€ìƒ‰í•´ë³´ì„¸ìš”.")

# ê²€ìƒ‰ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ë•Œë§Œ ì´ë¯¸ì§€ í‘œì‹œ
if not st.session_state['search_executed']:
    st.image("https://raw.githubusercontent.com/Chojueun/Architecture_Searching/main/cover.png")

# ì‚¬ì´ë“œë°”ì— ê²€ìƒ‰ ì¡°ê±´ ë°°ì¹˜
with st.sidebar:
    st.header("ê²€ìƒ‰ ì¡°ê±´")
    source = st.radio("ê²€ìƒ‰í•  ì±„ë„ì„ ì„ íƒí•˜ì„¸ìš”:", ("YouTube", "ë‰´ìŠ¤"), key='source')
    domain = st.selectbox("ê±´ì¶•/ê±´ì„¤ ë„ë©”ì¸ ì„ íƒ", list(ARCH_DOMAINS.keys()), key='domain')
    additional_query = st.text_input("ì¶”ê°€ ê²€ìƒ‰ì–´ (ì„ íƒ ì‚¬í•­)", key="additional_query")
    period = st.selectbox("ì¡°íšŒ ê¸°ê°„", ["ëª¨ë‘", "ìµœê·¼ 1ì¼", "ìµœê·¼ 1ì£¼ì¼", "ìµœê·¼ 1ê°œì›”", "ìµœê·¼ 3ê°œì›”", "ìµœê·¼ 6ê°œì›”", "ìµœê·¼ 1ë…„"], index=2, key='period')
    st.button("ê²€ìƒ‰ ì‹¤í–‰", on_click=execute_search)

# ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
if st.session_state['search_executed']:
    source = st.session_state['source']

    if source == "YouTube":
        st.subheader(f"ğŸ¦ ê²€ìƒ‰ëœ YouTube ì˜ìƒ")
        for video in st.session_state.search_results['videos']:
            col1, col2 = st.columns([1, 2])
            with col1:
                st.image(video['snippet']['thumbnails']['medium']['url'], use_container_width=True)
            with col2:
                st.subheader(video['snippet']['title'])
                st.markdown(f"**ì±„ë„ëª…:** {video['snippet']['channelTitle']}")
                st.write(video['snippet']['description'])
                video_url = f"https://www.youtube.com/watch?v={video['id']['videoId']}"
                st.markdown(f"[ì˜ìƒ ë³´ê¸°]({video_url})")
                
                video_id = video['id']['videoId']
                video_title = video['snippet']['title']
                if st.button(f"ğŸ“‹ ìš”ì•½ ë³´ê³ ì„œ ìš”ì²­", key=f"summarize_{video_id}"):
                    with st.spinner("ì˜ìƒì„ ìš”ì•½í•˜ëŠ” ì¤‘..."):
                        summary = summarize_video(video_id, video_title)
                        st.session_state.summary = summary
            st.divider()
    
    elif source == "ë‰´ìŠ¤":
        st.subheader(f"ğŸ“° ê²€ìƒ‰ëœ ë‰´ìŠ¤ ê¸°ì‚¬")
        for i, article in enumerate(st.session_state.search_results['news']):
            st.subheader(article['title'])
            st.markdown(f"**ì¶œì²˜:** {article['source']['name']}")
            st.write(article['description'])
            st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({article['url']})")
            st.divider()
    
    # ìš”ì•½ ê²°ê³¼ í‘œì‹œ ë° ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    st.markdown('<div class="fixed-footer">', unsafe_allow_html=True)
    col1, col2 = st.columns([0.85, 0.15])  # ì—´ì„ ë¹„ìœ¨ë¡œ ë¶„í• 
    with col1:
        if source == "YouTube":
            st.subheader("ğŸ“‹ ì˜ìƒ ìš”ì•½ ë³´ê³ ì„œ")
        elif source == "ë‰´ìŠ¤":
            st.subheader("ğŸ“‹ ë‰´ìŠ¤ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ")

    with col2:
        if st.session_state.summary:
            download_summary_file(st.session_state.summary)
    
    if st.session_state.summary:
        st.markdown(st.session_state.summary, unsafe_allow_html=True)
    else:
        st.write("ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìš”ì•½í•  í•­ëª©ì„ ì„ íƒí•˜ê±°ë‚˜, ê²€ìƒ‰ ì¡°ê±´ì„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”.")
    st.markdown('</div>', unsafe_allow_html=True)

# ì£¼ì˜ì‚¬í•­ ë° ì•ˆë‚´
st.sidebar.markdown("---")
st.sidebar.markdown("**ì•ˆë‚´ì‚¬í•­:**")
st.sidebar.markdown("- ì´ ì„œë¹„ìŠ¤ëŠ” Google AI Studio API, YouTube Data API, Google Search API, Yahoo Financeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
st.sidebar.markdown("- ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆê³¼ ë³µì¡ë„ì— ë”°ë¼ ì²˜ë¦¬ ì‹œê°„ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
st.sidebar.markdown("- ì €ì‘ê¶Œ ë³´í˜¸ë¥¼ ìœ„í•´ ê°œì¸ì ì¸ ìš©ë„ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
